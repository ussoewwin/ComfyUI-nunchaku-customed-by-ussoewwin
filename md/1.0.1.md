# PuLID Fix for Nunchaku FLUX in ComfyUI: Complete Explanation

## Overview

5 files modified (3 site-packages, 2 custom nodes) to fix 10 errors encountered when integrating PuLID (Personalized Identity) functionality with Nunchaku FLUX models in ComfyUI.

---

## Chapter 1: transformer_flux.py

**File:** `python_embeded/Lib/site-packages/nunchaku/models/transformers/transformer_flux.py`

**Purpose:** Fixes compatibility with ComfyUI 0.3.68 by adding new parameters to the `forward()` method and handling LoRA loading errors when PuLID is applied.

### Changes Made

#### 1. Add parameters to forward() method (Errors 1-3 fix)

**Cause:** ComfyUI 0.3.68 passes `control`, `transformer_options`, and `y` to `forward()`, but Nunchaku's implementation did not define them.

**Fix:** Add to the `forward()` signature:
- `control: Optional[torch.Tensor] = None`
- `transformer_options: Optional[Dict[str, Any]] = None`
- `y: Optional[torch.Tensor] = None`

These are not used internally, but including them prevents `TypeError: got an unexpected keyword argument`.

#### 2. LoRA loading with strict=False (Error 8 fix)

**Cause:** When PuLID is applied, `pulid_ca` keys are added dynamically. LoRA files do not include `pulid_ca` keys, so loading with `strict=True` fails with `RuntimeError: Error(s) in loading state_dict Missing pulid_ca keys`.

**Fix:** Use `strict=False` in `_update_unquantized_part_lora_params()`. This ignores missing `pulid_ca` keys (preserving original values) while applying LoRA parameters correctly.

#### 3. Safe __del__ method (Error 10 fix)

**Cause:** If an exception occurs during `__init__`, `self.m` may not be set, causing `AttributeError` in `__del__` when calling `self.m.reset()`.

**Fix:** Add `hasattr` checks and exception handling in `__del__` to safely release resources.

### Key Code

```python
def forward(
    self,
    hidden_states: torch.Tensor,
    encoder_hidden_states: torch.Tensor = None,
    pooled_projections: torch.Tensor = None,
    timestep: torch.LongTensor = None,
    img_ids: torch.Tensor = None,
    txt_ids: torch.Tensor = None,
    guidance: torch.Tensor = None,
    joint_attention_kwargs: Optional[Dict[str, Any]] = None,
    controlnet_block_samples=None,
    controlnet_single_block_samples=None,
    return_dict: bool = True,
    controlnet_blocks_repeat: bool = False,
    # Added for ComfyUI 0.3.68 compatibility
    control: Optional[torch.Tensor] = None,
    transformer_options: Optional[Dict[str, Any]] = None,
    y: Optional[torch.Tensor] = None,
) -> Union[torch.FloatTensor, Transformer2DModelOutput]:
```

```python
diff = strength * (lora_b @ lora_a)
new_state_dict[k] = v + diff
else:
    new_state_dict[k] = v
self.load_state_dict(new_state_dict, strict=False)
```

---

## Chapter 2: flux.py (caching adapters)

**File:** `python_embeded/Lib/site-packages/nunchaku/caching/diffusers_adapters/flux.py`

**Purpose:** Fixes argument duplication errors when the caching mechanism is applied.

### Changes Made

#### Remove duplicate encoder_hidden_states (Error 4 fix)

**Cause:** `encoder_hidden_states` was passed both as a positional argument and in `kwargs`, causing `TypeError: got multiple values for argument 'encoder_hidden_states'`.

**Fix:** In `new_forward()`, if `encoder_hidden_states` exists in both positional args and `kwargs`, remove it from the positional args to eliminate the duplication.

### Key Code

```python
def new_forward(self, *args, **kwargs):
    cache_context = get_current_cache_context()
    # Remove duplicate encoder_hidden_states
    if len(args) > 0 and 'encoder_hidden_states' in kwargs:
        args = args[1:]  # Remove encoder_hidden_states from positional args
    if cache_context is not None:
        with (
            unittest.mock.patch.object(self, "transformer_blocks", cached_transformer_blocks),
            unittest.mock.patch.object(self, "single_transformer_blocks", dummy_single_transformer_blocks),
        ):
            transformer._is_cached = True
            transformer.cached_transformer_blocks = cached_transformer_blocks
            transformer.single_transformer_blocks = dummy_single_transformer_blocks
            return original_forward(*args, **kwargs)
```

---

## Chapter 3: pipeline_flux_pulid.py

**File:** `python_embeded/Lib/site-packages/nunchaku/pipeline/pipeline_flux_pulid.py`

**Purpose:** Prevents crashes when facexlib fails to detect faces, allowing graceful fallback.

### Changes Made

#### Change RuntimeError to warning log (facexlib failure handling)

**Cause:** When facexlib's `align_warp_face()` failed to detect faces, the code raised `RuntimeError("facexlib align face fail")`, causing the entire pipeline to crash.

**Fix:** Replace the `raise RuntimeError` with a warning log and return `(None, None)`. This allows the ComfyUI node to handle the failure gracefully by logging a warning and returning the original model without PuLID applied.

### Key Code

```python
if len(self.face_helper.cropped_faces) == 0:
    # raise RuntimeError("facexlib align face fail")
    logger.warning("facexlib align face fail (no face detected), skipping this image.")
    return None, None
```

**Note:** This change works together with the fix in `nodes/models/pulid.py` (Chapter 4), where the ComfyUI node checks if `id_embedding is None` and returns the original model with a warning message.

---

## Chapter 4: pulid.py (custom nodes)

**File:** `ComfyUI/custom_nodes/ComfyUI-nunchaku/nodes/models/pulid.py`

**Purpose:** Fixes model copying and device configuration issues when applying PuLID.

### Changes Made

#### 1. Change copy.deepcopy() to model.clone() (Errors 6, 9 fix)

**Cause:** `copy.deepcopy()` cannot serialize `QuantizedFluxModel` (C extension), causing `TypeError: cannot pickle 'nunchaku._C.QuantizedFluxModel' object`.

**Fix:** Replace `copy.deepcopy(model)` with `model.clone()`. This copies the ModelPatcher structure only, while `QuantizedFluxModel` is shared by reference. This also resolves `AttributeError: 'NoneType' object has no attribute 'model'` (Error 9), which occurred because `copy.deepcopy()` failed and returned `None`.

#### 2. Graceful handling when face detection fails

Works with the fix in Chapter 3. When `get_id_embedding()` returns `None`, log a warning and return the original model.

### Key Code

```python
# Instead of deepcopy, we directly modify the existing wrapper
# and create a new ModelPatcher with the modified wrapper
# Clone the model's ModelPatcher to avoid modifying the original
ret_model = model.clone()

# Get the wrapper from the cloned model
ret_model_wrapper = ret_model.model.diffusion_model
assert isinstance(ret_model_wrapper, ComfyFluxWrapper), \
    f"Expected ComfyFluxWrapper, got {type(ret_model_wrapper).__name__}"
```

```python
id_embedding, _ = pulid_pipline.get_id_embedding(single_image)
if id_embedding is not None:
    all_embeddings.append(id_embedding)

if not all_embeddings:
    logger.warning("Nunchaku PuLID: No face detected in any of the images. Skipping PuLID.")
    return (model,)
```

---

## Chapter 5: flux.py (wrappers)

**File:** `ComfyUI/custom_nodes/ComfyUI-nunchaku/wrappers/flux.py`

**Purpose:** Fixes argument name mismatch between Nunchaku's `time_text_embed` method and ComfyUI's calling convention.

### Changes Made

#### Dynamic patching of time_text_embed argument name (Error 7 fix)

**Cause:** Nunchaku's `NunchakuFluxTransformer2dModel` calls `time_text_embed(timestep, pooled_projections)` (plural), but `CombinedTimestepGuidanceTextProjEmbeddings.forward()` expects `forward(timestep, guidance=None, pooled_projection=pooled_projections)` (singular). This mismatch caused argument name errors.

**Fix:** In `ComfyFluxWrapper.forward()`, temporarily patch `time_text_embed.forward` at call time. The patched function accepts `pooled_projections` and passes it as `pooled_projection` internally. Restore the original method after processing.

### Key Code

```python
# Patch time_text_embed to fix argument name issue (if it exists)
# Nunchaku's NunchakuFluxTransformer2dModel calls:
#   self.time_text_embed(timestep, pooled_projections)
# But CombinedTimestepGuidanceTextProjEmbeddings.forward expects:
#   forward(timestep, guidance=None, pooled_projection=pooled_projections)
original_forward = None
if self._original_time_text_embed is not None:
    original_forward = self._original_time_text_embed.forward
    def patched_forward(timestep, guidance=None, pooled_projections=None):
        return original_forward(timestep, guidance=guidance, pooled_projection=pooled_projections)
    self._original_time_text_embed.forward = patched_forward

try:
    with cache_context(self._cache_context):
        if self.customized_forward is None:
            out = model(...)
        else:
            out = self.customized_forward(...)
finally:
    # Restore original method (if it exists)
    if self._original_time_text_embed is not None:
        self._original_time_text_embed.forward = original_forward
```

**Note:** The same patching logic is implemented for both the cache-enabled path (274-305 lines) and the cache-disabled path (307-345 lines).

---

## Summary

### Modified Files

#### Site-packages side (3 files)

1. `python_embeded/Lib/site-packages/nunchaku/models/transformers/transformer_flux.py`
   - Added parameters to `forward()` method
   - Changed LoRA loading to use `strict=False`
   - Added safe `__del__` method

2. `python_embeded/Lib/site-packages/nunchaku/caching/diffusers_adapters/flux.py`
   - Removed duplicate `encoder_hidden_states` argument

3. `python_embeded/Lib/site-packages/nunchaku/pipeline/pipeline_flux_pulid.py`
   - Changed `RuntimeError` to warning log for facexlib failure

#### Custom nodes side (2 files)

4. `ComfyUI/custom_nodes/ComfyUI-nunchaku/nodes/models/pulid.py`
   - Changed `copy.deepcopy()` to `model.clone()`
   - Added graceful handling for face detection failure

5. `ComfyUI/custom_nodes/ComfyUI-nunchaku/wrappers/flux.py`
   - Added dynamic patching for `time_text_embed` argument name

**Total: 5 files modified** (addressing 10 errors documented in memory)

